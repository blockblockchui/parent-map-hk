#!/usr/bin/env python3
"""
ä½æˆæœ¬åœ°é»æœé›†è…³æœ¬ - ä¸ä½¿ç”¨ LLMï¼Œåªç”¨ RSS + HTTP æª¢æŸ¥
Usage: python3 ingest_lowcost.py [--dry-run]
"""

import os
import sys
import asyncio
import uuid
import re
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin
from dotenv import load_dotenv

load_dotenv()

# Check required packages
try:
    import feedparser
    import httpx
    import yaml
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¥—ä»¶: {e}")
    print("è«‹åŸ·è¡Œ: pip3 install --break-system-packages feedparser httpx pyyaml beautifulsoup4")
    sys.exit(1)

print("=" * 70)
print("ğŸš€ ä½æˆæœ¬åœ°é»æœé›† (Low Cost Pipeline)")
print("=" * 70)

# Load config
config_path = Path(__file__).parent / "config" / "sources_lowcost.yaml"
if not config_path.exists():
    print(f"âŒ Config file not found: {config_path}")
    sys.exit(1)

with open(config_path) as f:
    config = yaml.safe_load(f)

# Parse arguments
dry_run = "--dry-run" in sys.argv

if dry_run:
    print("\nğŸ” DRY RUN æ¨¡å¼ - ä¸æœƒå¯«å…¥ Google Sheets")

# Load sources
sources = [s for s in config.get('sources', []) if s.get('enabled', True)]
print(f"\nğŸ“¡ å•Ÿç”¨å˜…ä¾†æº: {len(sources)}")
for s in sources:
    print(f"  â€¢ {s['name']} ({s['type']})")

# Google Sheets setup
print("\nğŸ“‹ é€£æ¥ Google Sheets...")
from google.oauth2.service_account import Credentials
import gspread

sheet_id = os.getenv("GOOGLE_SHEETS_ID")
creds_path = Path(__file__).parent / "credentials.json"

credentials = Credentials.from_service_account_file(
    str(creds_path),
    scopes=["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
)
client = gspread.authorize(credentials)
spreadsheet = client.open_by_key(sheet_id)

try:
    worksheet = spreadsheet.worksheet("Places")
    print(f"âœ“ å·²é€£æ¥: {worksheet.title}")
except gspread.WorksheetNotFound:
    worksheet = spreadsheet.add_worksheet("Places", rows=1000, cols=40)
    print(f"âœ“ å·²å‰µå»º: Places")

# Get existing places for deduplication
print("\nğŸ“Š ç²å–ç¾æœ‰åœ°é»...")
existing = worksheet.get_all_records()
existing_names = {(row.get('name', ''), row.get('district', '')) for row in existing}
print(f"  ç¾æœ‰: {len(existing)} å€‹åœ°é»")

# Results
found_places = []

def extract_real_url_from_bing(url):
    """Extract real URL from Bing redirect URL"""
    if 'bing.com/news/apiclick.aspx' in url:
        import urllib.parse
        parsed = urllib.parse.urlparse(url)
        params = urllib.parse.parse_qs(parsed.query)
        if 'url' in params:
            return urllib.parse.unquote(params['url'][0])
    return url

async def fetch_url(url, timeout=10):
    """Fetch URL with timeout"""
    try:
        # Handle Bing redirect URLs
        url = extract_real_url_from_bing(url)
        
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True) as client:
            response = await client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (compatible; ParentMapBot/1.0)"
            })
            return response
    except Exception as e:
        return None

def extract_place_info(title, summary, url):
    """Extract place info from title and summary (without LLM)"""
    text = f"{title} {summary}"
    
    # Handle Bing redirect URLs
    real_url = extract_real_url_from_bing(url)
    
    # Pattern matching for common formats
    place = {
        'name': title.split('|')[0].split('â€“')[0].split('-')[0].strip()[:50],
        'category': 'playhouse',
        'indoor': True,
        'district': '',
        'address': '',
        'description': summary[:200] if summary else title[:200],
        'website_url': real_url,
        'source_url': real_url,
    }
    
    # Try to extract district
    districts = ['ç£ä»”', 'éŠ…é‘¼ç£', 'ä¸­ç’°', 'å°–æ²™å’€', 'æ—ºè§’', 'æ²™ç”°', 'èƒç£', 'ä¹é¾åŸ', 
                 'å°‡è»æ¾³', 'å¤§åŸ”', 'å±¯é–€', 'å…ƒæœ—', 'å¤©æ°´åœ', 'é’è¡£', 'è‘µæ¶Œ', 'è§€å¡˜']
    for d in districts:
        if d in text:
            place['district'] = d
            break
    
    # Detect category
    if any(k in text for k in ['åšç‰©é¤¨', 'ç§‘å­¸é¤¨', 'å¤ªç©ºé¤¨']):
        place['category'] = 'museum'
    elif any(k in text for k in ['å…¬åœ’', 'æ¿•åœ°']):
        place['category'] = 'park'
        place['indoor'] = False
    elif any(k in text for k in ['é¤å»³', 'cafÃ©', 'Cafe']):
        place['category'] = 'restaurant'
    
    # Detect indoor/outdoor
    if any(k in text for k in ['å®¤å¤–', 'æˆ¶å¤–', 'å…¬åœ’']):
        place['indoor'] = False
    
    return place

def check_already_exists(name, district):
    """Check if place already exists"""
    return (name, district) in existing_names

async def process_rss_source(source):
    """Process RSS source"""
    print(f"\nğŸ“° è™•ç†: {source['name']}")
    
    try:
        feed = feedparser.parse(source['url'])
        print(f"  æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")
        
        new_count = 0
        for entry in feed.entries[:10]:  # åªè™•ç†æœ€è¿‘ 10 ç¯‡
            title = entry.get('title', '')
            summary = entry.get('summary', entry.get('description', ''))
            link = entry.get('link', '')
            
            # Check keywords - must contain venue-related terms
            text = f"{title} {summary}".lower()
            
            # Must contain at least one venue indicator
            venue_terms = ['æ¨‚åœ’', 'éŠæ¨‚å ´', 'playhouse', 'playroom', 'åšç‰©é¤¨', 'ç§‘å­¸é¤¨', 'å¤ªç©ºé¤¨', 'å…¬åœ’', 'æ¢ç´¢é¤¨', 'è¦ªå­é¤¨', 'å…’ç«¥', 'æ”¾é›»']
            if not any(term in title for term in venue_terms):
                continue
            
            # Skip news articles about parenting advice, celebrity news, etc.
            skip_terms = ['é¬§ä»”', 'æ¹Šä»”', 'æ•™é¤Š', 'å°ˆè¨ª', 'æ˜æ˜Ÿ', 'è—äºº', 'å¤«å¦»', 'å©šå§»', 'æ‡·å­•', 'ç”Ÿä»”', 'åˆ†å¨©', 'æ¹ŠB']
            if any(term in title for term in skip_terms):
                print(f"  â­ï¸  è·³ééåœ°é»æ–‡ç« : {title[:40]}...")
                continue
            
            # Extract place info
            place = extract_place_info(title, summary, link)
            
            if not place['name']:
                continue
            
            # Check duplicate
            if check_already_exists(place['name'], place['district']):
                print(f"  âš ï¸  å·²å­˜åœ¨: {place['name']}")
                continue
            
            # HTTP check (cheap validation)
            print(f"  ğŸ” æª¢æŸ¥: {place['name']}")
            response = await fetch_url(link)
            if response and response.status_code == 200:
                place['http_ok'] = True
                new_count += 1
                found_places.append(place)
                print(f"  âœ“ æœ‰æ•ˆ: {place['name']}")
            else:
                print(f"  âŒ ç„¡æ³•è¨ªå•: {place['name']}")
            
            # Rate limiting
            await asyncio.sleep(1)
        
        print(f"  âœ“ æ–°å¢: {new_count} å€‹")
        
    except Exception as e:
        print(f"  âŒ éŒ¯èª¤: {e}")

async def main():
    # Process all sources
    for source in sources:
        if source['type'] == 'rss':
            await process_rss_source(source)
        
        # Add delay between sources
        await asyncio.sleep(2)
    
    print(f"\n{'=' * 70}")
    print(f"ğŸ“Š æœé›†å®Œæˆ: {len(found_places)} å€‹æ–°åœ°é»")
    print(f"{'=' * 70}")
    
    if not found_places:
        print("\nâ„¹ï¸ æ²’æœ‰æ‰¾åˆ°æ–°åœ°é»")
        return
    
    # Show found places
    print("\nğŸ“ æ‰¾åˆ°å˜…åœ°é»:")
    for i, p in enumerate(found_places[:5], 1):
        print(f"  {i}. {p['name']} ({p['district'] or 'æœªçŸ¥åœ°å€'})")
    if len(found_places) > 5:
        print(f"  ... é‚„æœ‰ {len(found_places) - 5} å€‹")
    
    if dry_run:
        print("\nğŸ” DRY RUN - ä¸æœƒå¯«å…¥ Google Sheets")
        return
    
    # Import to Google Sheets
    print("\nğŸ“ åŒ¯å…¥ Google Sheets...")
    
    SHEET_COLUMNS = [
        "place_id", "slug", "name", "name_en", "region", "district", "address", "lat", "lng",
        "geocode_confidence", "category", "indoor", "age_min", "age_max", "price_tier",
        "price_description", "description", "tips", "facilities", "opening_hours",
        "website_url", "facebook_url", "instagram_url", "google_maps_url", "status",
        "validation_stage", "confidence", "risk_tier", "evidence_urls", "evidence_snippets",
        "source_urls", "published_at", "updated_at", "last_checked_at", "next_check_at",
        "checked_at", "review_owner", "review_due_at", "resolution", "false_alarm_reason"
    ]
    
    imported = 0
    for place in found_places:
        try:
            row_data = [
                str(uuid.uuid4())[:8],  # place_id
                place['name'].lower().replace(' ', '-')[:50],  # slug
                place['name'],  # name
                '',  # name_en
                'hk-island' if not place['district'] else ('kowloon' if place['district'] in ['å°–æ²™å’€', 'æ—ºè§’', 'ä¹é¾åŸ'] else 'nt'),  # region
                place['district'],  # district
                place.get('address', ''),  # address
                '',  # lat
                '',  # lng
                'rss_extracted',  # geocode_confidence
                place['category'],  # category
                'TRUE' if place['indoor'] else 'FALSE',  # indoor
                '',  # age_min
                '',  # age_max
                '',  # price_tier
                '',  # price_description
                place['description'],  # description
                '',  # tips
                '',  # facilities
                'è«‹æŸ¥è©¢å®˜ç¶²',  # opening_hours
                place['website_url'],  # website_url
                '',  # facebook_url
                '',  # instagram_url
                '',  # google_maps_url
                'PendingReview',  # status
                'rss_extracted',  # validation_stage
                50,  # confidence
                'medium',  # risk_tier
                '',  # evidence_urls
                '',  # evidence_snippets
                place['source_url'],  # source_urls
                datetime.now().isoformat(),  # published_at
                datetime.now().isoformat(),  # updated_at
                '',  # last_checked_at
                '',  # next_check_at
                datetime.now().strftime('%Y-%m-%d'),  # checked_at
                '',  # review_owner
                '',  # review_due_at
                '',  # resolution
                '',  # false_alarm_reason
            ]
            
            worksheet.append_row(row_data)
            imported += 1
            print(f"  âœ“ {place['name']}")
            
        except Exception as e:
            print(f"  âŒ {place['name']}: {e}")
    
    print(f"\nâœ… åŒ¯å…¥å®Œæˆ: {imported} å€‹åœ°é»")
    print(f"\nğŸ“Š Sheet URL:")
    print(f"https://docs.google.com/spreadsheets/d/{sheet_id}/edit")
    print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
    print(f"  1. æª¢æŸ¥ 'PendingReview' ç‹€æ…‹å˜…åœ°é»")
    print(f"  2. æ‰‹å‹•è£œå……åº§æ¨™åŒè©³æƒ…")
    print(f"  3. åŸ·è¡Œ export_json.py æ›´æ–°å‰ç«¯")

# Run
if __name__ == "__main__":
    asyncio.run(main())
